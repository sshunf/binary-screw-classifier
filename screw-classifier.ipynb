{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79745b99-42d4-43d4-9d05-480746a632c8",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Screw Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077c056-53cd-4969-b791-7606d74c13bd",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1) Download and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ebdf7-b25e-432c-ac0c-19473f04a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1c450-000b-43ef-a200-cdaaf5e60871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import os\n",
    "import zipfile\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdbb52-2ff2-4479-afc6-d93859278998",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2) Download MVTec Srews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef1e19-5504-4380-be90-1d26540f30c8",
   "metadata": {},
   "source": [
    "Run the cell below to download the dataset from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274df2a3-73a6-493d-ba4a-007de855d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "  #Download dataset from google drive url\n",
    "  url = 'https://drive.google.com/uc?id=11ozVs6zByFjs9viD3VIIP6qKFgjZwv9E'\n",
    "  output_filename = 'screws.zip'\n",
    "\n",
    "  # Check if dataset has already\n",
    "  if os.path.exists('archive'):\n",
    "    print(\"Dataset already exists.\")\n",
    "    return\n",
    "\n",
    "  # Download the dataset\n",
    "  print(\"Downloading dataset...\")\n",
    "  gdown.download(url, output=output_filename, quiet=False)\n",
    "\n",
    "  # Extract the contents\n",
    "  with zipfile.ZipFile(output_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "  os.remove(output_filename) # Delete the zip file\n",
    "  print(\"Dataset downloaded!\")\n",
    "\n",
    "\n",
    "download_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc64b92-eb6d-49c8-8784-4b7ac2d6bee2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3) Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97933da-d057-4b1c-8572-fab6de951efa",
   "metadata": {},
   "source": [
    "Run the cell below to load the dataset from the downloaded file and create batches of 32. The data loaders are then stored into the variables `train_loader` and `test_loader` for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cecad9-ebf6-4964-90b3-bd96b218cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def prepare_dataset(data_path: str, batch_size: int, train_split: float, test_split=None, transform=None):\n",
    "  # torch.manual_seed(14)\n",
    "\n",
    "  if not test_split:\n",
    "    test_split = 1 - train_split\n",
    "\n",
    "  # Define our transfrom\n",
    "  dataset = datasets.ImageFolder(root=data_path,\n",
    "                                    transform=transform,\n",
    "                                    target_transform=None)\n",
    "\n",
    "  train_size = int(train_split * len(dataset)) # compute train set size\n",
    "  test_size = int(test_split * len(dataset)) # computer test set size\n",
    "  train_data, test_data = random_split(dataset, [train_size, test_size]) # randomly split data into training and test sets\n",
    "\n",
    "  # Create Dataloaders (batches)\n",
    "  train_loader = DataLoader(dataset=train_data,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=os.cpu_count(),\n",
    "                                shuffle=True)\n",
    "  test_loader = DataLoader(dataset=test_data,\n",
    "                               batch_size=batch_size,\n",
    "                               num_workers=os.cpu_count(),\n",
    "                               shuffle=False)\n",
    "\n",
    "  return train_loader, test_loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  DATA_PATH = '/content/archive/train'\n",
    "\n",
    "   #### Define hyperparameters ####\n",
    "  BATCH_SIZE = 32\n",
    "  TRAIN_SPLIT = 0.7\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize((224, 224)), # resize image to feed into resnet\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "  ############### Prepare Dataset ###############\n",
    "  train_loader, test_loader = prepare_dataset(data_path=DATA_PATH, batch_size=BATCH_SIZE, train_split=TRAIN_SPLIT, transform=transform)\n",
    "\n",
    "  print(f\"Number of training batches: {len(train_loader)}\") # print number of train batches\n",
    "  print(f\"Number of testing batches: {len(test_loader)}\") # print number of test batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df08b8-8abc-494f-8ef1-308c79d1fcc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4) Create Model and Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864e14b-0e63-4b82-bbb9-2c34f9d2114e",
   "metadata": {},
   "source": [
    "#### 4.1) Define Model `screw_classifier()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89dc53-4ff4-4b5c-b35c-01cd4193a5ec",
   "metadata": {},
   "source": [
    "Run the cell below to create the model class and an instance named `model` (resnet 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce33d5f-b177-41b9-9247-d0043a38af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "def screw_classifier():\n",
    "  # torch.manual_seed(14)\n",
    "  resnet = resnet50(weights=ResNet50_Weights.DEFAULT) # pre-trained performs better than untrained weights\n",
    "\n",
    "  num_features = resnet.fc.in_features\n",
    "  resnet.fc = torch.nn.Sequential(\n",
    "      torch.nn.Linear(num_features, 512),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Dropout(0.2), # regularization to prevent overfitting by randomly setting a fraction of input units to zero during training.\n",
    "      torch.nn.Linear(512, 2), # output layer with 2 neurons for binary classification\n",
    "  )\n",
    "  return resnet\n",
    "\n",
    "# Define our model\n",
    "model = screw_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab869235-aade-4d1d-ad83-1705e32d7ca0",
   "metadata": {},
   "source": [
    "### 4.2) Define Evaluation Metrics `evaluate_model()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915eb30-7bcf-42c7-84f0-4a1f209e33d6",
   "metadata": {},
   "source": [
    "**Run** the cell below to define the `evaluate_model()` function. This function will measure a evaluate a model based on the following metrics: precision, recall, accuracy and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28907dfd-c1d3-4af6-828d-1ab7b668b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_positives, false_positives):\n",
    "  return true_positives / (true_positives + false_positives + 1e-10)\n",
    "\n",
    "def recall(true_positives, false_negatives):\n",
    "  return true_positives / (true_positives + false_negatives + 1e-10)\n",
    "\n",
    "def accuracy(true_positives, true_negatives, false_positives, false_negatives):\n",
    "  return (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "\n",
    "def f1_score(p, rec):\n",
    "  return (2 * (p * rec) / (p + rec + 1e-10))\n",
    "\n",
    "def evaluate_model(true_positives, true_negatives, false_positives, false_negatives, epoch=None):\n",
    "  p = precision(true_positives, false_positives)\n",
    "  rec = recall(true_positives, false_negatives)\n",
    "  acc = accuracy(true_positives, true_negatives, false_positives, false_negatives)\n",
    "  f1 = (2 * (p * rec) / (p + rec + 1e-10))\n",
    "\n",
    "  if epoch == None:\n",
    "    print(f\"Precision: {p:3f} | recall: {rec:.3f} | accuracy: {acc:.0%} | f1_score: {f1:.3f}\")\n",
    "    return\n",
    "\n",
    "  print(f\"Epoch: {epoch + 1} | precision: {p:3f} | recall: {rec:.3f} | accuracy: {acc:.0%} | f1_score: {f1:.3f}\")\n",
    "  return p, rec, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04707e9f-255d-4035-9eff-f59a671c0994",
   "metadata": {},
   "source": [
    "### 4.3) Define the `test_model()` function which will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeed76e-52b1-4a2c-9e94-8b5d6785003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  all_labels = []\n",
    "  all_preds = []\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          outputs = model(inputs)\n",
    "          _, y_preds = outputs.max(1)\n",
    "          all_labels.extend(labels.cpu().numpy())\n",
    "          all_preds.extend(y_preds.cpu().numpy())\n",
    "\n",
    "          true_positives += (labels * y_preds).sum()\n",
    "          true_negatives += ((1 - labels) * (1 - y_preds)).sum()\n",
    "          false_positives += ((1 - labels) * y_preds).sum()\n",
    "          false_negatives += (labels * (1 - y_preds)).sum()\n",
    "          evaluate_model(true_positives, true_negatives, false_positives, false_negatives)\n",
    "          return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ad647-527e-436c-bbdb-839e0beca292",
   "metadata": {},
   "source": [
    "**[Optional Step]** - Run the cell below to test the resnet 50 with its pre-trained weights before training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ddf88-d53b-4f39-904f-ddb84a516bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2e959-8a25-4935-98f0-94da05912def",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5) Train & Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6361d54-e140-4d19-82c3-f5a3dfa6ad3f",
   "metadata": {},
   "source": [
    "### 5.1) Begin Training Below `train_model()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc7eb-f10f-47e3-afe3-cebf4fd6bfc6",
   "metadata": {},
   "source": [
    "Run the cell below to train the model. Feel free to edit any hyperparameters in the function call at the very bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfce97-d0aa-4641-9696-ff57934e5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, learning_rate):\n",
    "  # torch.manual_seed(14)\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  model.to(device)\n",
    "\n",
    "  weights = torch.tensor([0.3, 0.7]) # adjusted weights for unbalanced datset\n",
    "  loss_fn = torch.nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  accuracies, f1_scores, precisions, recalls, num_epochs, training_loss = [], [], [], [], [], []\n",
    "  num_epochs = epochs\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      model.train()  # Set the model to train mode\n",
    "\n",
    "      ######### Training loop ########\n",
    "      running_loss = 0.0\n",
    "      for inputs, labels in train_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          y_preds = model(inputs) # Forward pass\n",
    "          loss = loss_fn(y_preds, labels) # Calculate loss\n",
    "          optimizer.zero_grad() # Backward pass and optimization\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          running_loss += loss.item()\n",
    "\n",
    "      # Calculate average training loss for this epoch\n",
    "      avg_loss = running_loss / len(train_loader.dataset)\n",
    "      print(f\"Epoch [{epoch + 1}/{epochs}] | Total loss: {running_loss} | Avg Loss: {avg_loss:.4f}\")\n",
    "      training_loss.append(running_loss)\n",
    "\n",
    "      ######## Evaluation loop ########\n",
    "      model.eval()\n",
    "      all_labels, all_preds = [], []\n",
    "      true_positives,true_negatives, false_positives, false_negatives = 0, 0, 0, 0\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for inputs, labels in test_loader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, y_preds = outputs.max(dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(y_preds.cpu().numpy())\n",
    "\n",
    "            true_positives += (labels * y_preds).sum()\n",
    "            true_negatives += ((1 - labels) * (1 - y_preds)).sum()\n",
    "            false_positives += ((1 - labels) * y_preds).sum()\n",
    "            false_negatives += (labels * (1 - y_preds)).sum()\n",
    "\n",
    "      # Evaluate model and record metrics\n",
    "      p, rec, acc, f1 = evaluate_model(true_positives, true_negatives, false_positives, false_negatives, epoch)\n",
    "      accuracies.append(acc)\n",
    "      f1_scores.append(f1)\n",
    "      precisions.append(p)\n",
    "      recalls.append(rec)\n",
    "\n",
    "\n",
    "  return accuracies, f1_scores, precisions, recalls, num_epochs, all_labels, all_preds, training_loss\n",
    "\n",
    "######### Train Model ##########\n",
    "accuracies, f1_scores, precisions, recalls, num_epochs, all_labels, all_preds, training_loss = train_model(model,\n",
    "                                                                                            train_loader,\n",
    "                                                                                            test_loader,\n",
    "                                                                                            epochs=30,\n",
    "                                                                                            learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14486f92-ad42-4281-be95-636d621ed937",
   "metadata": {},
   "source": [
    "### 5.2) Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cc611-0a47-4a27-bc24-b7ce9f48221c",
   "metadata": {},
   "source": [
    "Run the cell below to save the trained model above to colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7681273-f7ce-4c46-bc2f-7a7d0b0255fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory = '/content/models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "# Define path and save model\n",
    "PATH = '/content/models/screw_classifier_model'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6941b3-aca5-4766-b18c-0e5a030a73b1",
   "metadata": {},
   "source": [
    "### 5.3) Visualize Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1547a03-9a73-43d7-9332-98be8ccfd874",
   "metadata": {},
   "source": [
    "Run the cell below to view the plots of accuracies, f1 scores, precisions, recalls, and total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72d61c-2137-4d67-a2c1-6a488934086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = np.array(torch.tensor(accuracies, device=\"cpu\"))\n",
    "f1_scores =  np.array(torch.tensor(f1_scores, device=\"cpu\"))\n",
    "precisions =  np.array(torch.tensor(precisions, device=\"cpu\"))\n",
    "recalls =  np.array(torch.tensor(recalls, device=\"cpu\"))\n",
    "total_loss =  np.array(torch.tensor(training_loss, device=\"cpu\"))\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(10, 20))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(epochs, accuracies, label='Accuracy')\n",
    "ax1.set_title('Accuracy vs. Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot F1 score\n",
    "ax2.plot(epochs, f1_scores, label='F1 Score')\n",
    "ax2.set_title('F1 Score vs. Epoch')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot Precision\n",
    "ax3.plot(epochs, precisions, label='Precision')\n",
    "ax3.set_title('Precision vs. Epoch')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.grid(True)\n",
    "\n",
    "# Plot Recall\n",
    "ax4.plot(epochs, recalls, label='Recall')\n",
    "ax4.set_title('Recall vs. Epoch')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Recall')\n",
    "ax4.grid(True)\n",
    "\n",
    "# Plot Total Loss\n",
    "ax5.plot(epochs, total_loss, label='Total Loss')\n",
    "ax5.set_title('Total Loss vs. Epoch')\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('Total Loss')\n",
    "ax5.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468b60c-31f3-4b39-bea7-3d53ddc1c603",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6) Test Model & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053a2d2-7f88-459a-865f-83bca9426cf5",
   "metadata": {},
   "source": [
    "### ***IMPORTANT***: [Optional] Skip this step if you are testing a model that was trained above. Run the cell below if you want to download and test a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391d66e-8ee3-4fc5-8754-3b69f61844d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/uc?export=download&id=1orhZKoJ80pVsYtmi1j3W8Fdt9IdqwWNs'\n",
    "output_filename = 'screws_classifier_trained'\n",
    "\n",
    "def download_trained_model():\n",
    "  if os.path.exists('screws_classifier_trained'):\n",
    "    print(\"Model already exists.\")\n",
    "    return\n",
    "\n",
    "    # Download the model\n",
    "  print(\"Downloading model...\")\n",
    "  gdown.download(url, output=output_filename, quiet=False)\n",
    "\n",
    "  print(\"Model downloaded!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  download_trained_model()\n",
    "  MODEL_PATH = '/content/screws_classifier_trained'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4407f2b-4ddc-468d-97b5-4034d5300db7",
   "metadata": {},
   "source": [
    "#### Test & Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e430f77-799d-4193-983d-6c8e7b921457",
   "metadata": {},
   "source": [
    "Note: Ensure functions `screw_classifier()`, `test_model()`, and `evaluate_model()` are defined from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81578ae9-24e3-47d8-8d3a-76a8fe4c5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Load our model\n",
    "if os.path.exists('/content/models/screw_classifier_model'):\n",
    "  MODEL_PATH = '/content/models/screw_classifier_model'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_to_test = screw_classifier()\n",
    "model_to_test.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(device)))\n",
    "\n",
    "# Test our model\n",
    "all_preds, all_labels = test_model(model_to_test, test_loader) # test_loader is the same test set as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a424e3f-fff5-471b-9e9e-0b4ea4024a00",
   "metadata": {},
   "source": [
    "#### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40aa04-bc21-4ed5-a297-ddf122261e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = torch.tensor(all_preds, device=\"cpu\")\n",
    "all_labels = torch.tensor(all_labels, device=\"cpu\")\n",
    "\n",
    "# Setup confusion matrix\n",
    "confmat = ConfusionMatrix(num_classes=2, task='binary')\n",
    "confmat_tensor = confmat(preds=all_preds, target=all_labels)\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(),\n",
    "    class_names=[\"undamaged\", \"damaged\"],\n",
    "    figsize=(5, 5)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
